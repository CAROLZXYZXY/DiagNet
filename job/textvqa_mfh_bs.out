parsed 34602 questions for train
restoring vocab
question vocab size: 8311
answer vocab size: 3000
2019-03-25 21:20:39	Train Epoch: [0]	Iter: 100	Loss: 6.8845
2019-03-25 21:21:05	Train Epoch: [0]	Iter: 200	Loss: 6.3091
2019-03-25 21:21:30	Train Epoch: [1]	Iter: 300	Loss: 6.1705
2019-03-25 21:21:56	Train Epoch: [1]	Iter: 400	Loss: 5.9703
2019-03-25 21:22:22	Train Epoch: [2]	Iter: 500	Loss: 5.8080
2019-03-25 21:22:48	Train Epoch: [2]	Iter: 600	Loss: 5.5885
2019-03-25 21:23:13	Train Epoch: [2]	Iter: 700	Loss: 5.3962
2019-03-25 21:23:39	Train Epoch: [3]	Iter: 800	Loss: 5.2043
2019-03-25 21:24:05	Train Epoch: [3]	Iter: 900	Loss: 5.0699
2019-03-25 21:24:31	Train Epoch: [4]	Iter: 1000	Loss: 4.9228
2019-03-25 21:24:57	Train Epoch: [4]	Iter: 1100	Loss: 4.7484
2019-03-25 21:25:23	Train Epoch: [5]	Iter: 1200	Loss: 4.6869
2019-03-25 21:25:49	Train Epoch: [5]	Iter: 1300	Loss: 4.5000
2019-03-25 21:26:15	Train Epoch: [5]	Iter: 1400	Loss: 4.4008
2019-03-25 21:26:41	Train Epoch: [6]	Iter: 1500	Loss: 4.2918
2019-03-25 21:27:07	Train Epoch: [6]	Iter: 1600	Loss: 4.1524
2019-03-25 21:27:33	Train Epoch: [7]	Iter: 1700	Loss: 4.0721
2019-03-25 21:27:59	Train Epoch: [7]	Iter: 1800	Loss: 3.9096
2019-03-25 21:28:25	Train Epoch: [7]	Iter: 1900	Loss: 3.8567
2019-03-25 21:28:51	Train Epoch: [8]	Iter: 2000	Loss: 3.6976
2019-03-25 21:29:17	Train Epoch: [8]	Iter: 2100	Loss: 3.6229
2019-03-25 21:29:42	Train Epoch: [9]	Iter: 2200	Loss: 3.5307
2019-03-25 21:30:08	Train Epoch: [9]	Iter: 2300	Loss: 3.3986
2019-03-25 21:30:35	Train Epoch: [9]	Iter: 2400	Loss: 3.3320
2019-03-25 21:31:00	Train Epoch: [10]	Iter: 2500	Loss: 3.1511
2019-03-25 21:31:25	Train Epoch: [10]	Iter: 2600	Loss: 3.1174
2019-03-25 21:31:51	Train Epoch: [11]	Iter: 2700	Loss: 2.9909
2019-03-25 21:32:17	Train Epoch: [11]	Iter: 2800	Loss: 2.8862
2019-03-25 21:32:43	Train Epoch: [12]	Iter: 2900	Loss: 2.8211
2019-03-25 21:33:09	Train Epoch: [12]	Iter: 3000	Loss: 2.6517
2019-03-25 21:33:35	Train Epoch: [12]	Iter: 3100	Loss: 2.6191
2019-03-25 21:34:00	Train Epoch: [13]	Iter: 3200	Loss: 2.4528
2019-03-25 21:34:26	Train Epoch: [13]	Iter: 3300	Loss: 2.3947
2019-03-25 21:34:52	Train Epoch: [14]	Iter: 3400	Loss: 2.3061
2019-03-25 21:35:18	Train Epoch: [14]	Iter: 3500	Loss: 2.1647
2019-03-25 21:35:44	Train Epoch: [14]	Iter: 3600	Loss: 2.1423
2019-03-25 21:36:10	Train Epoch: [15]	Iter: 3700	Loss: 1.9569
2019-03-25 21:36:36	Train Epoch: [15]	Iter: 3800	Loss: 1.9345
2019-03-25 21:37:02	Train Epoch: [16]	Iter: 3900	Loss: 1.8090
2019-03-25 21:37:28	Train Epoch: [16]	Iter: 4000	Loss: 1.7347
2019-03-25 21:37:54	Train Epoch: [17]	Iter: 4100	Loss: 1.6840
2019-03-25 21:38:20	Train Epoch: [17]	Iter: 4200	Loss: 1.5373
2019-03-25 21:38:46	Train Epoch: [17]	Iter: 4300	Loss: 1.5263
2019-03-25 21:39:12	Train Epoch: [18]	Iter: 4400	Loss: 1.3924
2019-03-25 21:39:38	Train Epoch: [18]	Iter: 4500	Loss: 1.3502
2019-03-25 21:40:04	Train Epoch: [19]	Iter: 4600	Loss: 1.2774
2019-03-25 21:40:30	Train Epoch: [19]	Iter: 4700	Loss: 1.1925
2019-03-25 21:40:56	Train Epoch: [19]	Iter: 4800	Loss: 1.1719
2019-03-25 21:41:22	Train Epoch: [20]	Iter: 4900	Loss: 1.0587
2019-03-25 21:41:48	Train Epoch: [20]	Iter: 5000	Loss: 1.0451
parsed 5000 questions for val
restoring vocab
question vocab size: 8311
answer vocab size: 3000
Validating...
20.00%40.00%60.00%80.00%0 questions were skipped in a single epoch
100.00%Deduping arr of len 5000
New len 5000
loading VQA annotations and questions into memory...
0:00:00.125090
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
computing accuracy
Finshed Percent: [--------------------] 0% Finshed Percent: [--------------------] 2% Finshed Percent: [#-------------------] 4% Finshed Percent: [#-------------------] 6% Finshed Percent: [##------------------] 8% Finshed Percent: [##------------------] 10% Finshed Percent: [##------------------] 12% Finshed Percent: [###-----------------] 14% Finshed Percent: [###-----------------] 16% Finshed Percent: [####----------------] 18% Finshed Percent: [####----------------] 20% Finshed Percent: [####----------------] 22% Finshed Percent: [#####---------------] 24% Finshed Percent: [#####---------------] 26% Finshed Percent: [######--------------] 28% Finshed Percent: [######--------------] 30% Finshed Percent: [######--------------] 32% Finshed Percent: [#######-------------] 34% Finshed Percent: [#######-------------] 36% Finshed Percent: [########------------] 38% Finshed Percent: [########------------] 40% Finshed Percent: [########------------] 42% Finshed Percent: [#########-----------] 44% Finshed Percent: [#########-----------] 46% Finshed Percent: [##########----------] 48% Finshed Percent: [##########----------] 50% Finshed Percent: [##########----------] 52% Finshed Percent: [###########---------] 54% Finshed Percent: [###########---------] 56% Finshed Percent: [############--------] 57% Finshed Percent: [############--------] 60% Finshed Percent: [############--------] 62% Finshed Percent: [#############-------] 64% Finshed Percent: [#############-------] 66% Finshed Percent: [##############------] 68% Finshed Percent: [##############------] 70% Finshed Percent: [##############------] 72% Finshed Percent: [###############-----] 74% Finshed Percent: [###############-----] 76% Finshed Percent: [################----] 78% Finshed Percent: [################----] 80% Finshed Percent: [################----] 82% Finshed Percent: [#################---] 84% Finshed Percent: [#################---] 86% Finshed Percent: [##################--] 88% Finshed Percent: [##################--] 90% Finshed Percent: [##################--] 92% Finshed Percent: [###################-] 94% Finshed Percent: [###################-] 96% Finshed Percent: [####################] 98% Done computing accuracy
Test loss: nan
Accuracy: 10.86
Test per ans {'other': 10.86}
Best accuracy of 10.86 was at iteration 5000
2019-03-25 21:42:23	Train Epoch: [21]	Iter: 5100	Loss: 0.9810
2019-03-25 21:42:48	Train Epoch: [21]	Iter: 5200	Loss: 0.9243
2019-03-25 21:43:14	Train Epoch: [22]	Iter: 5300	Loss: 0.9009
2019-03-25 21:43:40	Train Epoch: [22]	Iter: 5400	Loss: 0.8133
2019-03-25 21:44:06	Train Epoch: [22]	Iter: 5500	Loss: 0.8138
2019-03-25 21:44:32	Train Epoch: [23]	Iter: 5600	Loss: 0.7295
2019-03-25 21:44:59	Train Epoch: [23]	Iter: 5700	Loss: 0.7214
2019-03-25 21:45:25	Train Epoch: [24]	Iter: 5800	Loss: 0.6873
2019-03-25 21:45:51	Train Epoch: [24]	Iter: 5900	Loss: 0.6324
2019-03-25 21:46:17	Train Epoch: [24]	Iter: 6000	Loss: 0.6342
2019-03-25 21:46:43	Train Epoch: [25]	Iter: 6100	Loss: 0.5811
2019-03-25 21:47:09	Train Epoch: [25]	Iter: 6200	Loss: 0.5599
2019-03-25 21:47:35	Train Epoch: [26]	Iter: 6300	Loss: 0.5254
2019-03-25 21:48:01	Train Epoch: [26]	Iter: 6400	Loss: 0.4984
2019-03-25 21:48:27	Train Epoch: [27]	Iter: 6500	Loss: 0.4914
2019-03-25 21:48:54	Train Epoch: [27]	Iter: 6600	Loss: 0.4483
2019-03-25 21:49:20	Train Epoch: [27]	Iter: 6700	Loss: 0.4347
2019-03-25 21:49:46	Train Epoch: [28]	Iter: 6800	Loss: 0.3995
2019-03-25 21:50:12	Train Epoch: [28]	Iter: 6900	Loss: 0.3893
2019-03-25 21:50:38	Train Epoch: [29]	Iter: 7000	Loss: 0.3867
2019-03-25 21:51:05	Train Epoch: [29]	Iter: 7100	Loss: 0.3526
2019-03-25 21:51:30	Train Epoch: [29]	Iter: 7200	Loss: 0.3544
2019-03-25 21:51:56	Train Epoch: [30]	Iter: 7300	Loss: 0.3174
2019-03-25 21:52:22	Train Epoch: [30]	Iter: 7400	Loss: 0.3192
2019-03-25 21:52:48	Train Epoch: [31]	Iter: 7500	Loss: 0.3016
2019-03-25 21:53:14	Train Epoch: [31]	Iter: 7600	Loss: 0.2814
2019-03-25 21:53:40	Train Epoch: [32]	Iter: 7700	Loss: 0.2879
2019-03-25 21:54:06	Train Epoch: [32]	Iter: 7800	Loss: 0.2642
2019-03-25 21:54:32	Train Epoch: [32]	Iter: 7900	Loss: 0.2579
2019-03-25 21:54:58	Train Epoch: [33]	Iter: 8000	Loss: 0.2418
2019-03-25 21:55:25	Train Epoch: [33]	Iter: 8100	Loss: 0.2426
2019-03-25 21:55:51	Train Epoch: [34]	Iter: 8200	Loss: 0.2325
2019-03-25 21:56:17	Train Epoch: [34]	Iter: 8300	Loss: 0.2213
2019-03-25 21:56:43	Train Epoch: [34]	Iter: 8400	Loss: 0.2165
2019-03-25 21:57:09	Train Epoch: [35]	Iter: 8500	Loss: 0.2072
2019-03-25 21:57:35	Train Epoch: [35]	Iter: 8600	Loss: 0.2003
2019-03-25 21:58:01	Train Epoch: [36]	Iter: 8700	Loss: 0.1947
2019-03-25 21:58:27	Train Epoch: [36]	Iter: 8800	Loss: 0.1840
2019-03-25 21:58:53	Train Epoch: [37]	Iter: 8900	Loss: 0.1876
2019-03-25 21:59:19	Train Epoch: [37]	Iter: 9000	Loss: 0.1765
2019-03-25 21:59:45	Train Epoch: [37]	Iter: 9100	Loss: 0.1723
2019-03-25 22:00:12	Train Epoch: [38]	Iter: 9200	Loss: 0.1686
2019-03-25 22:00:38	Train Epoch: [38]	Iter: 9300	Loss: 0.1635
2019-03-25 22:01:04	Train Epoch: [39]	Iter: 9400	Loss: 0.1593
2019-03-25 22:01:30	Train Epoch: [39]	Iter: 9500	Loss: 0.1532
2019-03-25 22:01:56	Train Epoch: [39]	Iter: 9600	Loss: 0.1539
2019-03-25 22:02:22	Train Epoch: [40]	Iter: 9700	Loss: 0.1481
2019-03-25 22:02:49	Train Epoch: [40]	Iter: 9800	Loss: 0.1408
2019-03-25 22:03:15	Train Epoch: [41]	Iter: 9900	Loss: 0.1426
2019-03-25 22:03:40	Train Epoch: [41]	Iter: 10000	Loss: 0.1350
parsed 5000 questions for val
restoring vocab
question vocab size: 8311
answer vocab size: 3000
Validating...
20.00%40.00%60.00%80.00%0 questions were skipped in a single epoch
100.00%Deduping arr of len 5000
New len 5000
loading VQA annotations and questions into memory...
0:00:00.070469
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
computing accuracy
Finshed Percent: [--------------------] 0% Finshed Percent: [--------------------] 2% Finshed Percent: [#-------------------] 4% Finshed Percent: [#-------------------] 6% Finshed Percent: [##------------------] 8% Finshed Percent: [##------------------] 10% Finshed Percent: [##------------------] 12% Finshed Percent: [###-----------------] 14% Finshed Percent: [###-----------------] 16% Finshed Percent: [####----------------] 18% Finshed Percent: [####----------------] 20% Finshed Percent: [####----------------] 22% Finshed Percent: [#####---------------] 24% Finshed Percent: [#####---------------] 26% Finshed Percent: [######--------------] 28% Finshed Percent: [######--------------] 30% Finshed Percent: [######--------------] 32% Finshed Percent: [#######-------------] 34% Finshed Percent: [#######-------------] 36% Finshed Percent: [########------------] 38% Finshed Percent: [########------------] 40% Finshed Percent: [########------------] 42% Finshed Percent: [#########-----------] 44% Finshed Percent: [#########-----------] 46% Finshed Percent: [##########----------] 48% Finshed Percent: [##########----------] 50% Finshed Percent: [##########----------] 52% Finshed Percent: [###########---------] 54% Finshed Percent: [###########---------] 56% Finshed Percent: [############--------] 57% Finshed Percent: [############--------] 60% Finshed Percent: [############--------] 62% Finshed Percent: [#############-------] 64% Finshed Percent: [#############-------] 66% Finshed Percent: [##############------] 68% Finshed Percent: [##############------] 70% Finshed Percent: [##############------] 72% Finshed Percent: [###############-----] 74% Finshed Percent: [###############-----] 76% Finshed Percent: [################----] 78% Finshed Percent: [################----] 80% Finshed Percent: [################----] 82% Finshed Percent: [#################---] 84% Finshed Percent: [#################---] 86% Finshed Percent: [##################--] 88% Finshed Percent: [##################--] 90% Finshed Percent: [##################--] 92% Finshed Percent: [###################-] 94% Finshed Percent: [###################-] 96% Finshed Percent: [####################] 98% Done computing accuracy
Test loss: nan
Accuracy: 10.62
Test per ans {'other': 10.62}
Best accuracy of 10.86 was at iteration 5000
2019-03-25 22:04:15	Train Epoch: [42]	Iter: 10100	Loss: 0.1339
2019-03-25 22:04:42	Train Epoch: [42]	Iter: 10200	Loss: 0.1321
2019-03-25 22:05:08	Train Epoch: [42]	Iter: 10300	Loss: 0.1290
2019-03-25 22:05:34	Train Epoch: [43]	Iter: 10400	Loss: 0.1274
2019-03-25 22:06:01	Train Epoch: [43]	Iter: 10500	Loss: 0.1242
2019-03-25 22:06:27	Train Epoch: [44]	Iter: 10600	Loss: 0.1227
2019-03-25 22:06:53	Train Epoch: [44]	Iter: 10700	Loss: 0.1155
2019-03-25 22:07:19	Train Epoch: [44]	Iter: 10800	Loss: 0.1179
2019-03-25 22:07:46	Train Epoch: [45]	Iter: 10900	Loss: 0.1127
2019-03-25 22:08:12	Train Epoch: [45]	Iter: 11000	Loss: 0.1124
2019-03-25 22:08:38	Train Epoch: [46]	Iter: 11100	Loss: 0.1097
2019-03-25 22:09:04	Train Epoch: [46]	Iter: 11200	Loss: 0.1069
2019-03-25 22:09:31	Train Epoch: [47]	Iter: 11300	Loss: 0.1059
2019-03-25 22:09:57	Train Epoch: [47]	Iter: 11400	Loss: 0.1038
2019-03-25 22:10:23	Train Epoch: [47]	Iter: 11500	Loss: 0.1062
2019-03-25 22:10:49	Train Epoch: [48]	Iter: 11600	Loss: 0.1017
2019-03-25 22:11:15	Train Epoch: [48]	Iter: 11700	Loss: 0.0997
2019-03-25 22:11:42	Train Epoch: [49]	Iter: 11800	Loss: 0.1005
2019-03-25 22:12:08	Train Epoch: [49]	Iter: 11900	Loss: 0.0993
2019-03-25 22:12:34	Train Epoch: [49]	Iter: 12000	Loss: 0.0955
2019-03-25 22:13:00	Train Epoch: [50]	Iter: 12100	Loss: 0.0953
2019-03-25 22:13:26	Train Epoch: [50]	Iter: 12200	Loss: 0.0921
2019-03-25 22:13:52	Train Epoch: [51]	Iter: 12300	Loss: 0.0943
2019-03-25 22:14:18	Train Epoch: [51]	Iter: 12400	Loss: 0.0917
2019-03-25 22:14:44	Train Epoch: [52]	Iter: 12500	Loss: 0.0876
2019-03-25 22:15:11	Train Epoch: [52]	Iter: 12600	Loss: 0.0880
2019-03-25 22:15:37	Train Epoch: [52]	Iter: 12700	Loss: 0.0855
2019-03-25 22:16:02	Train Epoch: [53]	Iter: 12800	Loss: 0.0871
2019-03-25 22:16:29	Train Epoch: [53]	Iter: 12900	Loss: 0.0853
2019-03-25 22:16:54	Train Epoch: [54]	Iter: 13000	Loss: 0.0834
2019-03-25 22:17:21	Train Epoch: [54]	Iter: 13100	Loss: 0.0813
2019-03-25 22:17:47	Train Epoch: [54]	Iter: 13200	Loss: 0.0850
2019-03-25 22:18:14	Train Epoch: [55]	Iter: 13300	Loss: 0.0812
2019-03-25 22:18:40	Train Epoch: [55]	Iter: 13400	Loss: 0.0802
2019-03-25 22:19:06	Train Epoch: [56]	Iter: 13500	Loss: 0.0821
2019-03-25 22:19:32	Train Epoch: [56]	Iter: 13600	Loss: 0.0774
2019-03-25 22:19:58	Train Epoch: [57]	Iter: 13700	Loss: 0.0792
2019-03-25 22:20:24	Train Epoch: [57]	Iter: 13800	Loss: 0.0759
2019-03-25 22:20:51	Train Epoch: [57]	Iter: 13900	Loss: 0.0778
2019-03-25 22:21:17	Train Epoch: [58]	Iter: 14000	Loss: 0.0781
2019-03-25 22:21:44	Train Epoch: [58]	Iter: 14100	Loss: 0.0766
2019-03-25 22:22:10	Train Epoch: [59]	Iter: 14200	Loss: 0.0755
2019-03-25 22:22:36	Train Epoch: [59]	Iter: 14300	Loss: 0.0754
2019-03-25 22:23:02	Train Epoch: [59]	Iter: 14400	Loss: 0.0733
2019-03-25 22:23:28	Train Epoch: [60]	Iter: 14500	Loss: 0.0742
2019-03-25 22:23:54	Train Epoch: [60]	Iter: 14600	Loss: 0.0735
2019-03-25 22:24:20	Train Epoch: [61]	Iter: 14700	Loss: 0.0705
2019-03-25 22:24:46	Train Epoch: [61]	Iter: 14800	Loss: 0.0723
2019-03-25 22:25:13	Train Epoch: [61]	Iter: 14900	Loss: 0.0715
2019-03-25 22:25:39	Train Epoch: [62]	Iter: 15000	Loss: 0.0699
parsed 5000 questions for val
restoring vocab
question vocab size: 8311
answer vocab size: 3000
Validating...
20.00%40.00%60.00%80.00%0 questions were skipped in a single epoch
100.00%Deduping arr of len 5000
New len 5000
loading VQA annotations and questions into memory...
0:00:00.064415
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
computing accuracy
Finshed Percent: [--------------------] 0% Finshed Percent: [--------------------] 2% Finshed Percent: [#-------------------] 4% Finshed Percent: [#-------------------] 6% Finshed Percent: [##------------------] 8% Finshed Percent: [##------------------] 10% Finshed Percent: [##------------------] 12% Finshed Percent: [###-----------------] 14% Finshed Percent: [###-----------------] 16% Finshed Percent: [####----------------] 18% Finshed Percent: [####----------------] 20% Finshed Percent: [####----------------] 22% Finshed Percent: [#####---------------] 24% Finshed Percent: [#####---------------] 26% Finshed Percent: [######--------------] 28% Finshed Percent: [######--------------] 30% Finshed Percent: [######--------------] 32% Finshed Percent: [#######-------------] 34% Finshed Percent: [#######-------------] 36% Finshed Percent: [########------------] 38% Finshed Percent: [########------------] 40% Finshed Percent: [########------------] 42% Finshed Percent: [#########-----------] 44% Finshed Percent: [#########-----------] 46% Finshed Percent: [##########----------] 48% Finshed Percent: [##########----------] 50% Finshed Percent: [##########----------] 52% Finshed Percent: [###########---------] 54% Finshed Percent: [###########---------] 56% Finshed Percent: [############--------] 57% Finshed Percent: [############--------] 60% Finshed Percent: [############--------] 62% Finshed Percent: [#############-------] 64% Finshed Percent: [#############-------] 66% Finshed Percent: [##############------] 68% Finshed Percent: [##############------] 70% Finshed Percent: [##############------] 72% Finshed Percent: [###############-----] 74% Finshed Percent: [###############-----] 76% Finshed Percent: [################----] 78% Finshed Percent: [################----] 80% Finshed Percent: [################----] 82% Finshed Percent: [#################---] 84% Finshed Percent: [#################---] 86% Finshed Percent: [##################--] 88% Finshed Percent: [##################--] 90% Finshed Percent: [##################--] 92% Finshed Percent: [###################-] 94% Finshed Percent: [###################-] 96% Finshed Percent: [####################] 98% Done computing accuracy
Test loss: nan
Accuracy: 10.64
Test per ans {'other': 10.64}
Best accuracy of 10.86 was at iteration 5000
2019-03-25 22:26:14	Train Epoch: [62]	Iter: 15100	Loss: 0.0704
2019-03-25 22:26:41	Train Epoch: [63]	Iter: 15200	Loss: 0.0690
2019-03-25 22:27:07	Train Epoch: [63]	Iter: 15300	Loss: 0.0703
2019-03-25 22:27:33	Train Epoch: [64]	Iter: 15400	Loss: 0.0687
2019-03-25 22:28:00	Train Epoch: [64]	Iter: 15500	Loss: 0.0691
2019-03-25 22:28:26	Train Epoch: [64]	Iter: 15600	Loss: 0.0683
2019-03-25 22:28:52	Train Epoch: [65]	Iter: 15700	Loss: 0.0674
2019-03-25 22:29:18	Train Epoch: [65]	Iter: 15800	Loss: 0.0674
2019-03-25 22:29:45	Train Epoch: [66]	Iter: 15900	Loss: 0.0672
2019-03-25 22:30:11	Train Epoch: [66]	Iter: 16000	Loss: 0.0666
2019-03-25 22:30:37	Train Epoch: [66]	Iter: 16100	Loss: 0.0668
2019-03-25 22:31:03	Train Epoch: [67]	Iter: 16200	Loss: 0.0687
2019-03-25 22:31:29	Train Epoch: [67]	Iter: 16300	Loss: 0.0666
2019-03-25 22:31:55	Train Epoch: [68]	Iter: 16400	Loss: 0.0659
2019-03-25 22:32:21	Train Epoch: [68]	Iter: 16500	Loss: 0.0656
2019-03-25 22:32:47	Train Epoch: [69]	Iter: 16600	Loss: 0.0658
2019-03-25 22:33:13	Train Epoch: [69]	Iter: 16700	Loss: 0.0628
2019-03-25 22:33:39	Train Epoch: [69]	Iter: 16800	Loss: 0.0635
2019-03-25 22:34:06	Train Epoch: [70]	Iter: 16900	Loss: 0.0643
2019-03-25 22:34:32	Train Epoch: [70]	Iter: 17000	Loss: 0.0628
2019-03-25 22:34:58	Train Epoch: [71]	Iter: 17100	Loss: 0.0624
2019-03-25 22:35:25	Train Epoch: [71]	Iter: 17200	Loss: 0.0625
2019-03-25 22:35:51	Train Epoch: [71]	Iter: 17300	Loss: 0.0619
2019-03-25 22:36:17	Train Epoch: [72]	Iter: 17400	Loss: 0.0624
2019-03-25 22:36:44	Train Epoch: [72]	Iter: 17500	Loss: 0.0617
2019-03-25 22:37:10	Train Epoch: [73]	Iter: 17600	Loss: 0.0608
2019-03-25 22:37:36	Train Epoch: [73]	Iter: 17700	Loss: 0.0620
2019-03-25 22:38:02	Train Epoch: [74]	Iter: 17800	Loss: 0.0616
2019-03-25 22:38:28	Train Epoch: [74]	Iter: 17900	Loss: 0.0600
2019-03-25 22:38:54	Train Epoch: [74]	Iter: 18000	Loss: 0.0572
2019-03-25 22:39:21	Train Epoch: [75]	Iter: 18100	Loss: 0.0591
2019-03-25 22:39:47	Train Epoch: [75]	Iter: 18200	Loss: 0.0596
2019-03-25 22:40:13	Train Epoch: [76]	Iter: 18300	Loss: 0.0593
2019-03-25 22:40:39	Train Epoch: [76]	Iter: 18400	Loss: 0.0593
2019-03-25 22:41:05	Train Epoch: [76]	Iter: 18500	Loss: 0.0586
2019-03-25 22:41:31	Train Epoch: [77]	Iter: 18600	Loss: 0.0572
2019-03-25 22:41:58	Train Epoch: [77]	Iter: 18700	Loss: 0.0593
2019-03-25 22:42:24	Train Epoch: [78]	Iter: 18800	Loss: 0.0565
2019-03-25 22:42:50	Train Epoch: [78]	Iter: 18900	Loss: 0.0583
2019-03-25 22:43:16	Train Epoch: [79]	Iter: 19000	Loss: 0.0581
2019-03-25 22:43:42	Train Epoch: [79]	Iter: 19100	Loss: 0.0588
2019-03-25 22:44:08	Train Epoch: [79]	Iter: 19200	Loss: 0.0583
2019-03-25 22:44:34	Train Epoch: [80]	Iter: 19300	Loss: 0.0571
2019-03-25 22:45:00	Train Epoch: [80]	Iter: 19400	Loss: 0.0583
2019-03-25 22:45:26	Train Epoch: [81]	Iter: 19500	Loss: 0.0558
2019-03-25 22:45:53	Train Epoch: [81]	Iter: 19600	Loss: 0.0564
2019-03-25 22:46:19	Train Epoch: [81]	Iter: 19700	Loss: 0.0553
2019-03-25 22:46:45	Train Epoch: [82]	Iter: 19800	Loss: 0.0567
2019-03-25 22:47:11	Train Epoch: [82]	Iter: 19900	Loss: 0.0551
2019-03-25 22:47:38	Train Epoch: [83]	Iter: 20000	Loss: 0.0560
parsed 5000 questions for val
restoring vocab
question vocab size: 8311
answer vocab size: 3000
Validating...
20.00%40.00%60.00%80.00%0 questions were skipped in a single epoch
100.00%Deduping arr of len 5000
New len 5000
loading VQA annotations and questions into memory...
0:00:00.065031
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
computing accuracy
Finshed Percent: [--------------------] 0% Finshed Percent: [--------------------] 2% Finshed Percent: [#-------------------] 4% Finshed Percent: [#-------------------] 6% Finshed Percent: [##------------------] 8% Finshed Percent: [##------------------] 10% Finshed Percent: [##------------------] 12% Finshed Percent: [###-----------------] 14% Finshed Percent: [###-----------------] 16% Finshed Percent: [####----------------] 18% Finshed Percent: [####----------------] 20% Finshed Percent: [####----------------] 22% Finshed Percent: [#####---------------] 24% Finshed Percent: [#####---------------] 26% Finshed Percent: [######--------------] 28% Finshed Percent: [######--------------] 30% Finshed Percent: [######--------------] 32% Finshed Percent: [#######-------------] 34% Finshed Percent: [#######-------------] 36% Finshed Percent: [########------------] 38% Finshed Percent: [########------------] 40% Finshed Percent: [########------------] 42% Finshed Percent: [#########-----------] 44% Finshed Percent: [#########-----------] 46% Finshed Percent: [##########----------] 48% Finshed Percent: [##########----------] 50% Finshed Percent: [##########----------] 52% Finshed Percent: [###########---------] 54% Finshed Percent: [###########---------] 56% Finshed Percent: [############--------] 57% Finshed Percent: [############--------] 60% Finshed Percent: [############--------] 62% Finshed Percent: [#############-------] 64% Finshed Percent: [#############-------] 66% Finshed Percent: [##############------] 68% Finshed Percent: [##############------] 70% Finshed Percent: [##############------] 72% Finshed Percent: [###############-----] 74% Finshed Percent: [###############-----] 76% Finshed Percent: [################----] 78% Finshed Percent: [################----] 80% Finshed Percent: [################----] 82% Finshed Percent: [#################---] 84% Finshed Percent: [#################---] 86% Finshed Percent: [##################--] 88% Finshed Percent: [##################--] 90% Finshed Percent: [##################--] 92% Finshed Percent: [###################-] 94% Finshed Percent: [###################-] 96% Finshed Percent: [####################] 98% Done computing accuracy
Test loss: nan
Accuracy: 11.11
Test per ans {'other': 11.11}
Best accuracy of 11.11 was at iteration 20000
2019-03-25 22:48:12	Train Epoch: [83]	Iter: 20100	Loss: 0.0503
2019-03-25 22:48:38	Train Epoch: [84]	Iter: 20200	Loss: 0.0439
2019-03-25 22:49:04	Train Epoch: [84]	Iter: 20300	Loss: 0.0379
2019-03-25 22:49:31	Train Epoch: [84]	Iter: 20400	Loss: 0.0364
2019-03-25 22:49:57	Train Epoch: [85]	Iter: 20500	Loss: 0.0355
2019-03-25 22:50:23	Train Epoch: [85]	Iter: 20600	Loss: 0.0346
2019-03-25 22:50:49	Train Epoch: [86]	Iter: 20700	Loss: 0.0348
2019-03-25 22:51:16	Train Epoch: [86]	Iter: 20800	Loss: 0.0341
2019-03-25 22:51:42	Train Epoch: [86]	Iter: 20900	Loss: 0.0339
2019-03-25 22:52:08	Train Epoch: [87]	Iter: 21000	Loss: 0.0335
2019-03-25 22:52:35	Train Epoch: [87]	Iter: 21100	Loss: 0.0341
2019-03-25 22:53:00	Train Epoch: [88]	Iter: 21200	Loss: 0.0340
2019-03-25 22:53:26	Train Epoch: [88]	Iter: 21300	Loss: 0.0347
2019-03-25 22:53:53	Train Epoch: [89]	Iter: 21400	Loss: 0.0337
2019-03-25 22:54:19	Train Epoch: [89]	Iter: 21500	Loss: 0.0339
2019-03-25 22:54:45	Train Epoch: [89]	Iter: 21600	Loss: 0.0350
2019-03-25 22:55:11	Train Epoch: [90]	Iter: 21700	Loss: 0.0345
2019-03-25 22:55:38	Train Epoch: [90]	Iter: 21800	Loss: 0.0340
2019-03-25 22:56:04	Train Epoch: [91]	Iter: 21900	Loss: 0.0343
2019-03-25 22:56:30	Train Epoch: [91]	Iter: 22000	Loss: 0.0339
2019-03-25 22:56:56	Train Epoch: [91]	Iter: 22100	Loss: 0.0341
2019-03-25 22:57:23	Train Epoch: [92]	Iter: 22200	Loss: 0.0339
2019-03-25 22:57:49	Train Epoch: [92]	Iter: 22300	Loss: 0.0338
2019-03-25 22:58:15	Train Epoch: [93]	Iter: 22400	Loss: 0.0333
2019-03-25 22:58:42	Train Epoch: [93]	Iter: 22500	Loss: 0.0338
2019-03-25 22:59:08	Train Epoch: [94]	Iter: 22600	Loss: 0.0333
2019-03-25 22:59:34	Train Epoch: [94]	Iter: 22700	Loss: 0.0341
2019-03-25 23:00:00	Train Epoch: [94]	Iter: 22800	Loss: 0.0327
2019-03-25 23:00:26	Train Epoch: [95]	Iter: 22900	Loss: 0.0328
2019-03-25 23:00:52	Train Epoch: [95]	Iter: 23000	Loss: 0.0331
2019-03-25 23:01:18	Train Epoch: [96]	Iter: 23100	Loss: 0.0338
2019-03-25 23:01:45	Train Epoch: [96]	Iter: 23200	Loss: 0.0327
2019-03-25 23:02:11	Train Epoch: [96]	Iter: 23300	Loss: 0.0330
2019-03-25 23:02:37	Train Epoch: [97]	Iter: 23400	Loss: 0.0326
2019-03-25 23:03:03	Train Epoch: [97]	Iter: 23500	Loss: 0.0330
2019-03-25 23:03:29	Train Epoch: [98]	Iter: 23600	Loss: 0.0327
2019-03-25 23:03:56	Train Epoch: [98]	Iter: 23700	Loss: 0.0336
2019-03-25 23:04:22	Train Epoch: [99]	Iter: 23800	Loss: 0.0326
2019-03-25 23:04:48	Train Epoch: [99]	Iter: 23900	Loss: 0.0328
2019-03-25 23:05:14	Train Epoch: [99]	Iter: 24000	Loss: 0.0322
2019-03-25 23:05:40	Train Epoch: [100]	Iter: 24100	Loss: 0.0325
2019-03-25 23:06:07	Train Epoch: [100]	Iter: 24200	Loss: 0.0317
2019-03-25 23:06:33	Train Epoch: [101]	Iter: 24300	Loss: 0.0325
2019-03-25 23:06:59	Train Epoch: [101]	Iter: 24400	Loss: 0.0324
2019-03-25 23:07:26	Train Epoch: [101]	Iter: 24500	Loss: 0.0317
2019-03-25 23:07:52	Train Epoch: [102]	Iter: 24600	Loss: 0.0319
2019-03-25 23:08:19	Train Epoch: [102]	Iter: 24700	Loss: 0.0315
2019-03-25 23:08:45	Train Epoch: [103]	Iter: 24800	Loss: 0.0322
2019-03-25 23:09:11	Train Epoch: [103]	Iter: 24900	Loss: 0.0320
2019-03-25 23:09:37	Train Epoch: [104]	Iter: 25000	Loss: 0.0317
parsed 5000 questions for val
restoring vocab
question vocab size: 8311
answer vocab size: 3000
Validating...
20.00%40.00%60.00%80.00%0 questions were skipped in a single epoch
100.00%Deduping arr of len 5000
New len 5000
loading VQA annotations and questions into memory...
0:00:00.066472
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
computing accuracy
Finshed Percent: [--------------------] 0% Finshed Percent: [--------------------] 2% Finshed Percent: [#-------------------] 4% Finshed Percent: [#-------------------] 6% Finshed Percent: [##------------------] 8% Finshed Percent: [##------------------] 10% Finshed Percent: [##------------------] 12% Finshed Percent: [###-----------------] 14% Finshed Percent: [###-----------------] 16% Finshed Percent: [####----------------] 18% Finshed Percent: [####----------------] 20% Finshed Percent: [####----------------] 22% Finshed Percent: [#####---------------] 24% Finshed Percent: [#####---------------] 26% Finshed Percent: [######--------------] 28% Finshed Percent: [######--------------] 30% Finshed Percent: [######--------------] 32% Finshed Percent: [#######-------------] 34% Finshed Percent: [#######-------------] 36% Finshed Percent: [########------------] 38% Finshed Percent: [########------------] 40% Finshed Percent: [########------------] 42% Finshed Percent: [#########-----------] 44% Finshed Percent: [#########-----------] 46% Finshed Percent: [##########----------] 48% Finshed Percent: [##########----------] 50% Finshed Percent: [##########----------] 52% Finshed Percent: [###########---------] 54% Finshed Percent: [###########---------] 56% Finshed Percent: [############--------] 57% Finshed Percent: [############--------] 60% Finshed Percent: [############--------] 62% Finshed Percent: [#############-------] 64% Finshed Percent: [#############-------] 66% Finshed Percent: [##############------] 68% Finshed Percent: [##############------] 70% Finshed Percent: [##############------] 72% Finshed Percent: [###############-----] 74% Finshed Percent: [###############-----] 76% Finshed Percent: [################----] 78% Finshed Percent: [################----] 80% Finshed Percent: [################----] 82% Finshed Percent: [#################---] 84% Finshed Percent: [#################---] 86% Finshed Percent: [##################--] 88% Finshed Percent: [##################--] 90% Finshed Percent: [##################--] 92% Finshed Percent: [###################-] 94% Finshed Percent: [###################-] 96% Finshed Percent: [####################] 98% Done computing accuracy
Test loss: nan
Accuracy: 11.25
Test per ans {'other': 11.25}
Best accuracy of 11.25 was at iteration 25000
2019-03-25 23:10:12	Train Epoch: [104]	Iter: 25100	Loss: 0.0323
2019-03-25 23:10:38	Train Epoch: [104]	Iter: 25200	Loss: 0.0322
2019-03-25 23:11:05	Train Epoch: [105]	Iter: 25300	Loss: 0.0314
2019-03-25 23:11:31	Train Epoch: [105]	Iter: 25400	Loss: 0.0318
2019-03-25 23:11:57	Train Epoch: [106]	Iter: 25500	Loss: 0.0313
2019-03-25 23:12:23	Train Epoch: [106]	Iter: 25600	Loss: 0.0318
2019-03-25 23:12:49	Train Epoch: [106]	Iter: 25700	Loss: 0.0320
2019-03-25 23:13:16	Train Epoch: [107]	Iter: 25800	Loss: 0.0315
2019-03-25 23:13:42	Train Epoch: [107]	Iter: 25900	Loss: 0.0310
2019-03-25 23:14:08	Train Epoch: [108]	Iter: 26000	Loss: 0.0315
2019-03-25 23:14:34	Train Epoch: [108]	Iter: 26100	Loss: 0.0310
2019-03-25 23:15:01	Train Epoch: [109]	Iter: 26200	Loss: 0.0310
2019-03-25 23:15:27	Train Epoch: [109]	Iter: 26300	Loss: 0.0312
2019-03-25 23:15:53	Train Epoch: [109]	Iter: 26400	Loss: 0.0316
2019-03-25 23:16:19	Train Epoch: [110]	Iter: 26500	Loss: 0.0308
2019-03-25 23:16:45	Train Epoch: [110]	Iter: 26600	Loss: 0.0309
2019-03-25 23:17:11	Train Epoch: [111]	Iter: 26700	Loss: 0.0317
2019-03-25 23:17:37	Train Epoch: [111]	Iter: 26800	Loss: 0.0309
2019-03-25 23:18:03	Train Epoch: [111]	Iter: 26900	Loss: 0.0305
2019-03-25 23:18:30	Train Epoch: [112]	Iter: 27000	Loss: 0.0298
2019-03-25 23:18:56	Train Epoch: [112]	Iter: 27100	Loss: 0.0307
2019-03-25 23:19:22	Train Epoch: [113]	Iter: 27200	Loss: 0.0305
2019-03-25 23:19:48	Train Epoch: [113]	Iter: 27300	Loss: 0.0292
2019-03-25 23:20:14	Train Epoch: [113]	Iter: 27400	Loss: 0.0303
2019-03-25 23:20:40	Train Epoch: [114]	Iter: 27500	Loss: 0.0311
2019-03-25 23:21:06	Train Epoch: [114]	Iter: 27600	Loss: 0.0306
2019-03-25 23:21:33	Train Epoch: [115]	Iter: 27700	Loss: 0.0298
2019-03-25 23:22:01	Train Epoch: [115]	Iter: 27800	Loss: 0.0300
2019-03-25 23:22:27	Train Epoch: [116]	Iter: 27900	Loss: 0.0303
2019-03-25 23:22:53	Train Epoch: [116]	Iter: 28000	Loss: 0.0297
2019-03-25 23:23:19	Train Epoch: [116]	Iter: 28100	Loss: 0.0297
2019-03-25 23:23:46	Train Epoch: [117]	Iter: 28200	Loss: 0.0307
2019-03-25 23:24:12	Train Epoch: [117]	Iter: 28300	Loss: 0.0302
2019-03-25 23:24:38	Train Epoch: [118]	Iter: 28400	Loss: 0.0304
2019-03-25 23:25:05	Train Epoch: [118]	Iter: 28500	Loss: 0.0299
2019-03-25 23:25:31	Train Epoch: [118]	Iter: 28600	Loss: 0.0303
2019-03-25 23:25:57	Train Epoch: [119]	Iter: 28700	Loss: 0.0298
2019-03-25 23:26:23	Train Epoch: [119]	Iter: 28800	Loss: 0.0298
2019-03-25 23:26:49	Train Epoch: [120]	Iter: 28900	Loss: 0.0296
2019-03-25 23:27:16	Train Epoch: [120]	Iter: 29000	Loss: 0.0294
2019-03-25 23:27:41	Train Epoch: [121]	Iter: 29100	Loss: 0.0294
2019-03-25 23:28:08	Train Epoch: [121]	Iter: 29200	Loss: 0.0294
2019-03-25 23:28:34	Train Epoch: [121]	Iter: 29300	Loss: 0.0297
2019-03-25 23:29:00	Train Epoch: [122]	Iter: 29400	Loss: 0.0295
2019-03-25 23:29:26	Train Epoch: [122]	Iter: 29500	Loss: 0.0294
2019-03-25 23:29:52	Train Epoch: [123]	Iter: 29600	Loss: 0.0304
2019-03-25 23:30:19	Train Epoch: [123]	Iter: 29700	Loss: 0.0300
2019-03-25 23:30:45	Train Epoch: [123]	Iter: 29800	Loss: 0.0297
2019-03-25 23:31:11	Train Epoch: [124]	Iter: 29900	Loss: 0.0293
2019-03-25 23:31:38	Train Epoch: [124]	Iter: 30000	Loss: 0.0289
parsed 5000 questions for val
restoring vocab
question vocab size: 8311
answer vocab size: 3000
Validating...
20.00%40.00%60.00%80.00%0 questions were skipped in a single epoch
100.00%Deduping arr of len 5000
New len 5000
loading VQA annotations and questions into memory...
0:00:00.068715
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
computing accuracy
Finshed Percent: [--------------------] 0% Finshed Percent: [--------------------] 2% Finshed Percent: [#-------------------] 4% Finshed Percent: [#-------------------] 6% Finshed Percent: [##------------------] 8% Finshed Percent: [##------------------] 10% Finshed Percent: [##------------------] 12% Finshed Percent: [###-----------------] 14% Finshed Percent: [###-----------------] 16% Finshed Percent: [####----------------] 18% Finshed Percent: [####----------------] 20% Finshed Percent: [####----------------] 22% Finshed Percent: [#####---------------] 24% Finshed Percent: [#####---------------] 26% Finshed Percent: [######--------------] 28% Finshed Percent: [######--------------] 30% Finshed Percent: [######--------------] 32% Finshed Percent: [#######-------------] 34% Finshed Percent: [#######-------------] 36% Finshed Percent: [########------------] 38% Finshed Percent: [########------------] 40% Finshed Percent: [########------------] 42% Finshed Percent: [#########-----------] 44% Finshed Percent: [#########-----------] 46% Finshed Percent: [##########----------] 48% Finshed Percent: [##########----------] 50% Finshed Percent: [##########----------] 52% Finshed Percent: [###########---------] 54% Finshed Percent: [###########---------] 56% Finshed Percent: [############--------] 57% Finshed Percent: [############--------] 60% Finshed Percent: [############--------] 62% Finshed Percent: [#############-------] 64% Finshed Percent: [#############-------] 66% Finshed Percent: [##############------] 68% Finshed Percent: [##############------] 70% Finshed Percent: [##############------] 72% Finshed Percent: [###############-----] 74% Finshed Percent: [###############-----] 76% Finshed Percent: [################----] 78% Finshed Percent: [################----] 80% Finshed Percent: [################----] 82% Finshed Percent: [#################---] 84% Finshed Percent: [#################---] 86% Finshed Percent: [##################--] 88% Finshed Percent: [##################--] 90% Finshed Percent: [##################--] 92% Finshed Percent: [###################-] 94% Finshed Percent: [###################-] 96% Finshed Percent: [####################] 98% Done computing accuracy
Test loss: nan
Accuracy: 11.15
Test per ans {'other': 11.15}
Best accuracy of 11.25 was at iteration 25000
2019-03-25 23:32:13	Train Epoch: [125]	Iter: 30100	Loss: 0.0297
2019-03-25 23:32:38	Train Epoch: [125]	Iter: 30200	Loss: 0.0300
2019-03-25 23:33:04	Train Epoch: [126]	Iter: 30300	Loss: 0.0293
2019-03-25 23:33:31	Train Epoch: [126]	Iter: 30400	Loss: 0.0295
2019-03-25 23:33:57	Train Epoch: [126]	Iter: 30500	Loss: 0.0301
2019-03-25 23:34:23	Train Epoch: [127]	Iter: 30600	Loss: 0.0297
2019-03-25 23:34:49	Train Epoch: [127]	Iter: 30700	Loss: 0.0289
2019-03-25 23:35:15	Train Epoch: [128]	Iter: 30800	Loss: 0.0284
2019-03-25 23:35:41	Train Epoch: [128]	Iter: 30900	Loss: 0.0288
2019-03-25 23:36:07	Train Epoch: [128]	Iter: 31000	Loss: 0.0289
2019-03-25 23:36:33	Train Epoch: [129]	Iter: 31100	Loss: 0.0291
2019-03-25 23:36:59	Train Epoch: [129]	Iter: 31200	Loss: 0.0292
2019-03-25 23:37:25	Train Epoch: [130]	Iter: 31300	Loss: 0.0286
2019-03-25 23:37:52	Train Epoch: [130]	Iter: 31400	Loss: 0.0286
2019-03-25 23:38:18	Train Epoch: [131]	Iter: 31500	Loss: 0.0283
2019-03-25 23:38:44	Train Epoch: [131]	Iter: 31600	Loss: 0.0283
2019-03-25 23:39:10	Train Epoch: [131]	Iter: 31700	Loss: 0.0285
2019-03-25 23:39:36	Train Epoch: [132]	Iter: 31800	Loss: 0.0281
2019-03-25 23:40:02	Train Epoch: [132]	Iter: 31900	Loss: 0.0289
2019-03-25 23:40:29	Train Epoch: [133]	Iter: 32000	Loss: 0.0285
2019-03-25 23:40:55	Train Epoch: [133]	Iter: 32100	Loss: 0.0285
2019-03-25 23:41:21	Train Epoch: [133]	Iter: 32200	Loss: 0.0283
2019-03-25 23:41:48	Train Epoch: [134]	Iter: 32300	Loss: 0.0282
2019-03-25 23:42:13	Train Epoch: [134]	Iter: 32400	Loss: 0.0280
2019-03-25 23:42:39	Train Epoch: [135]	Iter: 32500	Loss: 0.0280
2019-03-25 23:43:05	Train Epoch: [135]	Iter: 32600	Loss: 0.0281
2019-03-25 23:43:32	Train Epoch: [136]	Iter: 32700	Loss: 0.0284
2019-03-25 23:43:58	Train Epoch: [136]	Iter: 32800	Loss: 0.0286
2019-03-25 23:44:24	Train Epoch: [136]	Iter: 32900	Loss: 0.0288
2019-03-25 23:44:50	Train Epoch: [137]	Iter: 33000	Loss: 0.0280
2019-03-25 23:45:17	Train Epoch: [137]	Iter: 33100	Loss: 0.0286
2019-03-25 23:45:43	Train Epoch: [138]	Iter: 33200	Loss: 0.0276
2019-03-25 23:46:08	Train Epoch: [138]	Iter: 33300	Loss: 0.0278
2019-03-25 23:46:34	Train Epoch: [138]	Iter: 33400	Loss: 0.0283
2019-03-25 23:47:00	Train Epoch: [139]	Iter: 33500	Loss: 0.0271
2019-03-25 23:47:27	Train Epoch: [139]	Iter: 33600	Loss: 0.0275
2019-03-25 23:47:53	Train Epoch: [140]	Iter: 33700	Loss: 0.0282
2019-03-25 23:48:19	Train Epoch: [140]	Iter: 33800	Loss: 0.0281
2019-03-25 23:48:45	Train Epoch: [141]	Iter: 33900	Loss: 0.0272
2019-03-25 23:49:11	Train Epoch: [141]	Iter: 34000	Loss: 0.0280
2019-03-25 23:49:37	Train Epoch: [141]	Iter: 34100	Loss: 0.0276
2019-03-25 23:50:03	Train Epoch: [142]	Iter: 34200	Loss: 0.0269
2019-03-25 23:50:29	Train Epoch: [142]	Iter: 34300	Loss: 0.0280
2019-03-25 23:50:55	Train Epoch: [143]	Iter: 34400	Loss: 0.0275
2019-03-25 23:51:22	Train Epoch: [143]	Iter: 34500	Loss: 0.0274
2019-03-25 23:51:47	Train Epoch: [143]	Iter: 34600	Loss: 0.0272
2019-03-25 23:52:14	Train Epoch: [144]	Iter: 34700	Loss: 0.0275
2019-03-25 23:52:39	Train Epoch: [144]	Iter: 34800	Loss: 0.0270
2019-03-25 23:53:05	Train Epoch: [145]	Iter: 34900	Loss: 0.0280
2019-03-25 23:53:31	Train Epoch: [145]	Iter: 35000	Loss: 0.0266
parsed 5000 questions for val
restoring vocab
question vocab size: 8311
answer vocab size: 3000
Validating...
20.00%40.00%60.00%80.00%0 questions were skipped in a single epoch
100.00%Deduping arr of len 5000
New len 5000
loading VQA annotations and questions into memory...
0:00:00.065694
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
computing accuracy
Finshed Percent: [--------------------] 0% Finshed Percent: [--------------------] 2% Finshed Percent: [#-------------------] 4% Finshed Percent: [#-------------------] 6% Finshed Percent: [##------------------] 8% Finshed Percent: [##------------------] 10% Finshed Percent: [##------------------] 12% Finshed Percent: [###-----------------] 14% Finshed Percent: [###-----------------] 16% Finshed Percent: [####----------------] 18% Finshed Percent: [####----------------] 20% Finshed Percent: [####----------------] 22% Finshed Percent: [#####---------------] 24% Finshed Percent: [#####---------------] 26% Finshed Percent: [######--------------] 28% Finshed Percent: [######--------------] 30% Finshed Percent: [######--------------] 32% Finshed Percent: [#######-------------] 34% Finshed Percent: [#######-------------] 36% Finshed Percent: [########------------] 38% Finshed Percent: [########------------] 40% Finshed Percent: [########------------] 42% Finshed Percent: [#########-----------] 44% Finshed Percent: [#########-----------] 46% Finshed Percent: [##########----------] 48% Finshed Percent: [##########----------] 50% Finshed Percent: [##########----------] 52% Finshed Percent: [###########---------] 54% Finshed Percent: [###########---------] 56% Finshed Percent: [############--------] 57% Finshed Percent: [############--------] 60% Finshed Percent: [############--------] 62% Finshed Percent: [#############-------] 64% Finshed Percent: [#############-------] 66% Finshed Percent: [##############------] 68% Finshed Percent: [##############------] 70% Finshed Percent: [##############------] 72% Finshed Percent: [###############-----] 74% Finshed Percent: [###############-----] 76% Finshed Percent: [################----] 78% Finshed Percent: [################----] 80% Finshed Percent: [################----] 82% Finshed Percent: [#################---] 84% Finshed Percent: [#################---] 86% Finshed Percent: [##################--] 88% Finshed Percent: [##################--] 90% Finshed Percent: [##################--] 92% Finshed Percent: [###################-] 94% Finshed Percent: [###################-] 96% Finshed Percent: [####################] 98% Done computing accuracy
Test loss: nan
Accuracy: 11.03
Test per ans {'other': 11.03}
Best accuracy of 11.25 was at iteration 25000
2019-03-25 23:54:06	Train Epoch: [146]	Iter: 35100	Loss: 0.0272
2019-03-25 23:54:32	Train Epoch: [146]	Iter: 35200	Loss: 0.0274
2019-03-25 23:54:58	Train Epoch: [146]	Iter: 35300	Loss: 0.0270
2019-03-25 23:55:24	Train Epoch: [147]	Iter: 35400	Loss: 0.0272
2019-03-25 23:55:50	Train Epoch: [147]	Iter: 35500	Loss: 0.0271
2019-03-25 23:56:16	Train Epoch: [148]	Iter: 35600	Loss: 0.0269
2019-03-25 23:56:42	Train Epoch: [148]	Iter: 35700	Loss: 0.0271
2019-03-25 23:57:08	Train Epoch: [148]	Iter: 35800	Loss: 0.0273
2019-03-25 23:57:34	Train Epoch: [149]	Iter: 35900	Loss: 0.0271
2019-03-25 23:58:01	Train Epoch: [149]	Iter: 36000	Loss: 0.0261
2019-03-25 23:58:27	Train Epoch: [150]	Iter: 36100	Loss: 0.0276
2019-03-25 23:58:53	Train Epoch: [150]	Iter: 36200	Loss: 0.0273
2019-03-25 23:59:19	Train Epoch: [151]	Iter: 36300	Loss: 0.0275
2019-03-25 23:59:45	Train Epoch: [151]	Iter: 36400	Loss: 0.0267
2019-03-26 00:00:11	Train Epoch: [151]	Iter: 36500	Loss: 0.0269
2019-03-26 00:00:38	Train Epoch: [152]	Iter: 36600	Loss: 0.0271
2019-03-26 00:01:04	Train Epoch: [152]	Iter: 36700	Loss: 0.0271
2019-03-26 00:01:30	Train Epoch: [153]	Iter: 36800	Loss: 0.0272
2019-03-26 00:01:56	Train Epoch: [153]	Iter: 36900	Loss: 0.0267
2019-03-26 00:02:22	Train Epoch: [153]	Iter: 37000	Loss: 0.0270
2019-03-26 00:02:49	Train Epoch: [154]	Iter: 37100	Loss: 0.0275
2019-03-26 00:03:15	Train Epoch: [154]	Iter: 37200	Loss: 0.0263
2019-03-26 00:03:41	Train Epoch: [155]	Iter: 37300	Loss: 0.0274
2019-03-26 00:04:06	Train Epoch: [155]	Iter: 37400	Loss: 0.0271
2019-03-26 00:04:32	Train Epoch: [156]	Iter: 37500	Loss: 0.0261
2019-03-26 00:04:58	Train Epoch: [156]	Iter: 37600	Loss: 0.0268
2019-03-26 00:05:24	Train Epoch: [156]	Iter: 37700	Loss: 0.0260
2019-03-26 00:05:50	Train Epoch: [157]	Iter: 37800	Loss: 0.0266
2019-03-26 00:06:16	Train Epoch: [157]	Iter: 37900	Loss: 0.0269
2019-03-26 00:06:42	Train Epoch: [158]	Iter: 38000	Loss: 0.0268
2019-03-26 00:07:08	Train Epoch: [158]	Iter: 38100	Loss: 0.0259
2019-03-26 00:07:34	Train Epoch: [158]	Iter: 38200	Loss: 0.0260
2019-03-26 00:08:00	Train Epoch: [159]	Iter: 38300	Loss: 0.0261
2019-03-26 00:08:27	Train Epoch: [159]	Iter: 38400	Loss: 0.0264
2019-03-26 00:08:53	Train Epoch: [160]	Iter: 38500	Loss: 0.0263
2019-03-26 00:09:19	Train Epoch: [160]	Iter: 38600	Loss: 0.0271
2019-03-26 00:09:45	Train Epoch: [161]	Iter: 38700	Loss: 0.0262
2019-03-26 00:10:11	Train Epoch: [161]	Iter: 38800	Loss: 0.0263
2019-03-26 00:10:36	Train Epoch: [161]	Iter: 38900	Loss: 0.0269
2019-03-26 00:11:02	Train Epoch: [162]	Iter: 39000	Loss: 0.0265
2019-03-26 00:11:28	Train Epoch: [162]	Iter: 39100	Loss: 0.0267
2019-03-26 00:11:55	Train Epoch: [163]	Iter: 39200	Loss: 0.0258
2019-03-26 00:12:21	Train Epoch: [163]	Iter: 39300	Loss: 0.0258
2019-03-26 00:12:46	Train Epoch: [163]	Iter: 39400	Loss: 0.0268
2019-03-26 00:13:13	Train Epoch: [164]	Iter: 39500	Loss: 0.0260
2019-03-26 00:13:39	Train Epoch: [164]	Iter: 39600	Loss: 0.0265
2019-03-26 00:14:05	Train Epoch: [165]	Iter: 39700	Loss: 0.0261
2019-03-26 00:14:31	Train Epoch: [165]	Iter: 39800	Loss: 0.0260
2019-03-26 00:14:57	Train Epoch: [166]	Iter: 39900	Loss: 0.0261
2019-03-26 00:15:23	Train Epoch: [166]	Iter: 40000	Loss: 0.0257
parsed 5000 questions for val
restoring vocab
question vocab size: 8311
answer vocab size: 3000
Validating...
20.00%40.00%60.00%80.00%0 questions were skipped in a single epoch
100.00%Deduping arr of len 5000
New len 5000
loading VQA annotations and questions into memory...
0:00:00.068306
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
computing accuracy
Finshed Percent: [--------------------] 0% Finshed Percent: [--------------------] 2% Finshed Percent: [#-------------------] 4% Finshed Percent: [#-------------------] 6% Finshed Percent: [##------------------] 8% Finshed Percent: [##------------------] 10% Finshed Percent: [##------------------] 12% Finshed Percent: [###-----------------] 14% Finshed Percent: [###-----------------] 16% Finshed Percent: [####----------------] 18% Finshed Percent: [####----------------] 20% Finshed Percent: [####----------------] 22% Finshed Percent: [#####---------------] 24% Finshed Percent: [#####---------------] 26% Finshed Percent: [######--------------] 28% Finshed Percent: [######--------------] 30% Finshed Percent: [######--------------] 32% Finshed Percent: [#######-------------] 34% Finshed Percent: [#######-------------] 36% Finshed Percent: [########------------] 38% Finshed Percent: [########------------] 40% Finshed Percent: [########------------] 42% Finshed Percent: [#########-----------] 44% Finshed Percent: [#########-----------] 46% Finshed Percent: [##########----------] 48% Finshed Percent: [##########----------] 50% Finshed Percent: [##########----------] 52% Finshed Percent: [###########---------] 54% Finshed Percent: [###########---------] 56% Finshed Percent: [############--------] 57% Finshed Percent: [############--------] 60% Finshed Percent: [############--------] 62% Finshed Percent: [#############-------] 64% Finshed Percent: [#############-------] 66% Finshed Percent: [##############------] 68% Finshed Percent: [##############------] 70% Finshed Percent: [##############------] 72% Finshed Percent: [###############-----] 74% Finshed Percent: [###############-----] 76% Finshed Percent: [################----] 78% Finshed Percent: [################----] 80% Finshed Percent: [################----] 82% Finshed Percent: [#################---] 84% Finshed Percent: [#################---] 86% Finshed Percent: [##################--] 88% Finshed Percent: [##################--] 90% Finshed Percent: [##################--] 92% Finshed Percent: [###################-] 94% Finshed Percent: [###################-] 96% Finshed Percent: [####################] 98% Done computing accuracy
Test loss: nan
Accuracy: 11.11
Test per ans {'other': 11.11}
Best accuracy of 11.25 was at iteration 25000
2019-03-26 00:15:58	Train Epoch: [166]	Iter: 40100	Loss: 0.0244
2019-03-26 00:16:24	Train Epoch: [167]	Iter: 40200	Loss: 0.0217
2019-03-26 00:16:50	Train Epoch: [167]	Iter: 40300	Loss: 0.0208
2019-03-26 00:17:16	Train Epoch: [168]	Iter: 40400	Loss: 0.0202
2019-03-26 00:17:43	Train Epoch: [168]	Iter: 40500	Loss: 0.0194
2019-03-26 00:18:09	Train Epoch: [168]	Iter: 40600	Loss: 0.0189
2019-03-26 00:18:35	Train Epoch: [169]	Iter: 40700	Loss: 0.0195
2019-03-26 00:19:01	Train Epoch: [169]	Iter: 40800	Loss: 0.0192
2019-03-26 00:19:27	Train Epoch: [170]	Iter: 40900	Loss: 0.0186
2019-03-26 00:19:53	Train Epoch: [170]	Iter: 41000	Loss: 0.0190
2019-03-26 00:20:20	Train Epoch: [170]	Iter: 41100	Loss: 0.0192
2019-03-26 00:20:46	Train Epoch: [171]	Iter: 41200	Loss: 0.0188
2019-03-26 00:21:12	Train Epoch: [171]	Iter: 41300	Loss: 0.0191
2019-03-26 00:21:38	Train Epoch: [172]	Iter: 41400	Loss: 0.0189
2019-03-26 00:22:04	Train Epoch: [172]	Iter: 41500	Loss: 0.0191
2019-03-26 00:22:30	Train Epoch: [173]	Iter: 41600	Loss: 0.0182
2019-03-26 00:22:56	Train Epoch: [173]	Iter: 41700	Loss: 0.0184
2019-03-26 00:23:23	Train Epoch: [173]	Iter: 41800	Loss: 0.0190
2019-03-26 00:23:49	Train Epoch: [174]	Iter: 41900	Loss: 0.0185
2019-03-26 00:24:15	Train Epoch: [174]	Iter: 42000	Loss: 0.0188
2019-03-26 00:24:41	Train Epoch: [175]	Iter: 42100	Loss: 0.0185
2019-03-26 00:25:08	Train Epoch: [175]	Iter: 42200	Loss: 0.0186
2019-03-26 00:25:34	Train Epoch: [175]	Iter: 42300	Loss: 0.0181
2019-03-26 00:26:00	Train Epoch: [176]	Iter: 42400	Loss: 0.0182
2019-03-26 00:26:27	Train Epoch: [176]	Iter: 42500	Loss: 0.0188
2019-03-26 00:26:53	Train Epoch: [177]	Iter: 42600	Loss: 0.0186
2019-03-26 00:27:19	Train Epoch: [177]	Iter: 42700	Loss: 0.0183
2019-03-26 00:27:45	Train Epoch: [178]	Iter: 42800	Loss: 0.0187
2019-03-26 00:28:12	Train Epoch: [178]	Iter: 42900	Loss: 0.0190
2019-03-26 00:28:38	Train Epoch: [178]	Iter: 43000	Loss: 0.0184
2019-03-26 00:29:04	Train Epoch: [179]	Iter: 43100	Loss: 0.0184
2019-03-26 00:29:30	Train Epoch: [179]	Iter: 43200	Loss: 0.0184
2019-03-26 00:29:57	Train Epoch: [180]	Iter: 43300	Loss: 0.0181
2019-03-26 00:30:23	Train Epoch: [180]	Iter: 43400	Loss: 0.0180
2019-03-26 00:30:49	Train Epoch: [180]	Iter: 43500	Loss: 0.0189
2019-03-26 00:31:15	Train Epoch: [181]	Iter: 43600	Loss: 0.0185
2019-03-26 00:31:41	Train Epoch: [181]	Iter: 43700	Loss: 0.0180
2019-03-26 00:32:08	Train Epoch: [182]	Iter: 43800	Loss: 0.0182
2019-03-26 00:32:34	Train Epoch: [182]	Iter: 43900	Loss: 0.0185
2019-03-26 00:33:00	Train Epoch: [183]	Iter: 44000	Loss: 0.0182
2019-03-26 00:33:26	Train Epoch: [183]	Iter: 44100	Loss: 0.0182
2019-03-26 00:33:53	Train Epoch: [183]	Iter: 44200	Loss: 0.0183
2019-03-26 00:34:19	Train Epoch: [184]	Iter: 44300	Loss: 0.0178
2019-03-26 00:34:45	Train Epoch: [184]	Iter: 44400	Loss: 0.0180
2019-03-26 00:35:11	Train Epoch: [185]	Iter: 44500	Loss: 0.0181
2019-03-26 00:35:37	Train Epoch: [185]	Iter: 44600	Loss: 0.0182
2019-03-26 00:36:03	Train Epoch: [185]	Iter: 44700	Loss: 0.0180
2019-03-26 00:36:29	Train Epoch: [186]	Iter: 44800	Loss: 0.0179
2019-03-26 00:36:55	Train Epoch: [186]	Iter: 44900	Loss: 0.0185
2019-03-26 00:37:22	Train Epoch: [187]	Iter: 45000	Loss: 0.0173
parsed 5000 questions for val
restoring vocab
question vocab size: 8311
answer vocab size: 3000
Validating...
20.00%40.00%60.00%80.00%0 questions were skipped in a single epoch
100.00%Deduping arr of len 5000
New len 5000
loading VQA annotations and questions into memory...
0:00:00.058866
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
computing accuracy
Finshed Percent: [--------------------] 0% Finshed Percent: [--------------------] 2% Finshed Percent: [#-------------------] 4% Finshed Percent: [#-------------------] 6% Finshed Percent: [##------------------] 8% Finshed Percent: [##------------------] 10% Finshed Percent: [##------------------] 12% Finshed Percent: [###-----------------] 14% Finshed Percent: [###-----------------] 16% Finshed Percent: [####----------------] 18% Finshed Percent: [####----------------] 20% Finshed Percent: [####----------------] 22% Finshed Percent: [#####---------------] 24% Finshed Percent: [#####---------------] 26% Finshed Percent: [######--------------] 28% Finshed Percent: [######--------------] 30% Finshed Percent: [######--------------] 32% Finshed Percent: [#######-------------] 34% Finshed Percent: [#######-------------] 36% Finshed Percent: [########------------] 38% Finshed Percent: [########------------] 40% Finshed Percent: [########------------] 42% Finshed Percent: [#########-----------] 44% Finshed Percent: [#########-----------] 46% Finshed Percent: [##########----------] 48% Finshed Percent: [##########----------] 50% Finshed Percent: [##########----------] 52% Finshed Percent: [###########---------] 54% Finshed Percent: [###########---------] 56% Finshed Percent: [############--------] 57% Finshed Percent: [############--------] 60% Finshed Percent: [############--------] 62% Finshed Percent: [#############-------] 64% Finshed Percent: [#############-------] 66% Finshed Percent: [##############------] 68% Finshed Percent: [##############------] 70% Finshed Percent: [##############------] 72% Finshed Percent: [###############-----] 74% Finshed Percent: [###############-----] 76% Finshed Percent: [################----] 78% Finshed Percent: [################----] 80% Finshed Percent: [################----] 82% Finshed Percent: [#################---] 84% Finshed Percent: [#################---] 86% Finshed Percent: [##################--] 88% Finshed Percent: [##################--] 90% Finshed Percent: [##################--] 92% Finshed Percent: [###################-] 94% Finshed Percent: [###################-] 96% Finshed Percent: [####################] 98% Done computing accuracy
Test loss: nan
Accuracy: 11.05
Test per ans {'other': 11.05}
Best accuracy of 11.25 was at iteration 25000
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
10565 questions were skipped in a single epoch
/data/home/wennyi/vqa-mfb.pytorch/models/mfh_baseline.py:60: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  prediction = F.log_softmax(prediction)
/data/home/wennyi/vqa-mfb.pytorch/utils/eval_utils.py:186: RuntimeWarning: Mean of empty slice.
  mean_testloss = np.array(testloss_list).mean()
/data/home/wennyi/opt/anaconda3/envs/mfb_py3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Traceback (most recent call last):
  File "train.py", line 136, in <module>
    main()
  File "train.py", line 132, in main
    train(opt, model, train_Loader, optimizer, writer, folder, train_Data.use_embed())
  File "train.py", line 84, in train
    exec_validation(model, opt, mode='test-dev', folder=folder, it=iter_idx)
  File "/data/home/wennyi/vqa-mfb.pytorch/utils/eval_utils.py", line 121, in exec_validation
    dp = VQADataProvider(opt, batchsize=opt.VAL_BATCH_SIZE, mode=mode)
  File "/data/home/wennyi/vqa-mfb.pytorch/utils/data_provider.py", line 25, in __init__
    self.qdic, self.adic = VQADataProvider.load_data(self.mode, self.exp_type)
  File "/data/home/wennyi/vqa-mfb.pytorch/utils/data_provider.py", line 176, in load_data
    assert data_split in config.DATA_PATHS[exp_type].keys(), 'unknown data split'
AssertionError: unknown data split
